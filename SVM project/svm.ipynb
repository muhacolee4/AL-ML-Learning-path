{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the required library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,Dense,MaxPool2D,Flatten\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir=r'C:\\Users\\Engr M2j\\Desktop\\Datasets\\svm\\dataset\\training_set'\n",
    "test_dir=r'C:\\Users\\Engr M2j\\Desktop\\Datasets\\svm\\dataset\\test_set'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code sets up data generators for training and testing a deep learning model on image data using Keras\n",
    "\n",
    "This code sets up data generators for training and testing a deep learning model on image data using Keras.\n",
    "\n",
    "The train_datagen object is an instance of the ImageDataGenerator class, which applies various data augmentation techniques to the training images to help the model generalize better to new images. The rescale parameter is used to rescale the pixel values of the images to be between 0 and 1, which is a common preprocessing step in deep learning. The shear_range, zoom_range, and horizontal_flip parameters are used to randomly apply shear, zoom, and horizontal flipping transformations to the images during training.\n",
    "\n",
    "The training_set object is created by calling the flow_from_directory method of the train_datagen object. This method reads images from a directory and generates batches of training data on the fly. The directory parameter specifies the path to the directory containing the training images, and the target_size parameter specifies the size to which the images should be resized. The batch_size parameter specifies the number of images to include in each batch, and the class_mode parameter specifies the type of labels to use. In this case, since we are doing binary classification, the class_mode is set to \"binary\".\n",
    "\n",
    "Similarly, the test_datagen object is an instance of ImageDataGenerator, but it does not apply any data augmentation techniques since we want to test the model on the original, unmodified images. The test_set object is created using the flow_from_directory method of the test_datagen object, and it generates batches of test data using the same parameters as the training_set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define data generators for training and testing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "train_datagen = ImageDataGenerator(rescale=(1/255.),shear_range = 0.2,zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "training_set = train_datagen.flow_from_directory(directory = train_dir,target_size=(64,64),\n",
    "                                                batch_size=32,\n",
    "                                                class_mode = \"binary\")\n",
    "test_datagen = ImageDataGenerator(rescale=(1/255.))\n",
    "test_set = test_datagen.flow_from_directory(directory = test_dir,target_size=(64,64),\n",
    "                                                batch_size=32,\n",
    "                                                class_mode = \"binary\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model development\n",
    "This code defines a Convolutional Neural Network (CNN) model using the Keras Sequential API. Here's what each line does:\n",
    "\n",
    "Create an empty Sequential model object.\n",
    "Add a 2D convolutional layer with 32 filters, a kernel size of 3x3, and a stride of 2. Set the padding to \"same\" and the activation function to ReLU. Specify the input shape as (64, 64, 3) for images with height and width of 64 and 3 color channels (RGB).\n",
    "Add a 2D max pooling layer with a pool size of 2x2 and a stride of 2.\n",
    "Add another 2D convolutional layer with the same parameters as the previous layer.\n",
    "Add another 2D max pooling layer with the same parameters as the previous layer.\n",
    "Flatten the output of the previous layer into a 1D vector.\n",
    "Add a dense (fully connected) layer with 128 neurons and ReLU activation.\n",
    "Add another dense layer with 1 neuron, linear activation, and L2 regularization with a regularization parameter of 0.01.\n",
    "This architecture is suitable for binary classification tasks, where the goal is to predict a binary outcome, such as whether an image contains a certain object or not. The input images are expected to have a size of 64x64 with 3 color channels (RGB), and the model outputs a single scalar value that represents the predicted outcome.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.regularizers import l2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32, padding = \"same\",activation = \"relu\",kernel_size=3, strides = 2,input_shape=(64,64,3)))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides = 2))\n",
    "\n",
    "model.add(Conv2D(filters = 32, padding = \"same\",activation = \"relu\",kernel_size=3))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides = 2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "\n",
    "#Output layer\n",
    "model.add(Dense(1,kernel_regularizer=l2(0.01),activation = \"linear\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.add(Dense(number_of_classes, kernel_regularizer=l2(0.01), activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='squared_hinge', metrics=['accuracy'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "250/250 [==============================] - 89s 346ms/step - loss: 1.2753 - accuracy: 0.4966 - val_loss: 1.2658 - val_accuracy: 0.4990\n",
      "Epoch 2/10\n",
      "250/250 [==============================] - 57s 227ms/step - loss: 1.2619 - accuracy: 0.4966 - val_loss: 1.2588 - val_accuracy: 0.5040\n",
      "Epoch 3/10\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 1.2567 - accuracy: 0.4899 - val_loss: 1.2549 - val_accuracy: 0.5085\n",
      "Epoch 4/10\n",
      "250/250 [==============================] - 54s 216ms/step - loss: 1.2536 - accuracy: 0.4979 - val_loss: 1.2526 - val_accuracy: 0.5065\n",
      "Epoch 5/10\n",
      "250/250 [==============================] - 56s 225ms/step - loss: 1.2519 - accuracy: 0.4889 - val_loss: 1.2513 - val_accuracy: 0.4940\n",
      "Epoch 6/10\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 1.2509 - accuracy: 0.4995 - val_loss: 1.2506 - val_accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "250/250 [==============================] - 59s 238ms/step - loss: 1.2504 - accuracy: 0.5000 - val_loss: 1.2502 - val_accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 1.2501 - accuracy: 0.5000 - val_loss: 1.2501 - val_accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 1.2500 - accuracy: 0.5000 - val_loss: 1.2500 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "250/250 [==============================] - 51s 206ms/step - loss: 1.2500 - accuracy: 0.5000 - val_loss: 1.2500 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x = training_set, validation_data = test_set, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
